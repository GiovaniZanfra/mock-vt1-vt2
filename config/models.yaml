# Espaço de busca com número de trials recomendado para Random Search

catboost:
  class_path: catboost.CatBoostRegressor
  n_trials: 50              # espaço relativamente grande (4 parâmetros)
  fixed_kwargs:
    silent: true
  params:
    iterations:
      _type: int
      low: 100
      high: 500
    depth:
      _type: categorical
      choices: [4, 6, 8]
    learning_rate:
      _type: loguniform
      low: 0.05
      high: 0.2
    l2_leaf_reg:
      _type: categorical
      choices: [1, 3, 5]

lgbm:
  class_path: lightgbm.LGBMRegressor
  n_trials: 20              # Busca mínima: apenas 20 trials
  params:
    learning_rate:
      _type: loguniform
      low: 0.01
      high: 0.2
    num_leaves:
      _type: categorical
      choices: [31, 63, 127]
  fixed_kwargs:
    max_depth: 10
    n_estimators: 100
    objective: regression
    metric: mae
    verbosity: -1
    boosting_type: gbdt

xgboost:
  class_path: xgboost.XGBRegressor
  n_trials: 50              # 5 parâmetros, mesmo raciocínio do CatBoost
  params:
    n_estimators:
      _type: int
      low: 50
      high: 300
    learning_rate:
      _type: loguniform
      low: 0.05
      high: 0.2
    max_depth:
      _type: categorical
      choices: [3, 6, 9]
    subsample:
      _type: uniform
      low: 0.6
      high: 1.0
    colsample_bytree:
      _type: categorical
      choices: [0.6, 0.8, 1.0]

knn:
  class_path: sklearn.neighbors.KNeighborsRegressor
  n_trials: 10              # 2 parâmetros, poucas escolhas
  params:
    n_neighbors:
      _type: categorical
      choices: [3, 6, 10]
    weights:
      _type: categorical
      choices: ["uniform", "distance"]

adaboost:
  class_path: sklearn.ensemble.AdaBoostRegressor
  n_trials: 20              # 2 parâmetros, orçamento moderado
  params:
    n_estimators:
      _type: int
      low: 50
      high: 200
    learning_rate:
      _type: loguniform
      low: 0.05
      high: 0.2

decision_tree:
  class_path: sklearn.tree.DecisionTreeRegressor
  n_trials: 15              # 3 escolhas categóricas, busca rápida
  params:
    max_depth:
      _type: categorical
      choices: [5, 10, 20]
    min_samples_split:
      _type: categorical
      choices: [2, 5, 10]
    min_samples_leaf:
      _type: categorical
      choices: [1, 3, 5]

random_forest:
  class_path: sklearn.ensemble.RandomForestRegressor
  n_trials: 30              # 3 parâmetros, treino um pouco mais custoso
  params:
    n_estimators:
      _type: int
      low: 50
      high: 200
    max_depth:
      _type: categorical
      choices: [5, 10, 20]
    max_features:
      _type: categorical
      choices: ["sqrt", "log2"]

extra_trees:
  class_path: sklearn.ensemble.ExtraTreesRegressor
  n_trials: 30              # similar ao RandomForest
  params:
    n_estimators:
      _type: int
      low: 50
      high: 200
    max_depth:
      _type: categorical
      choices: [5, 10, 20]
    max_features:
      _type: categorical
      choices: ["sqrt", "log2"]

linear_regression:
  class_path: sklearn.linear_model.LinearRegression
  n_trials: 1               # sem hiperparâmetros, basta 1 trial
  params: {}

ridge:
  class_path: sklearn.linear_model.Ridge
  n_trials: 8               # 4 valores de α, cobrir todos no random search
  params:
    alpha:
      _type: categorical
      choices: [0.01, 0.1, 1.0, 10.0]

lasso:
  class_path: sklearn.linear_model.Lasso
  n_trials: 8               # idem Ridge
  params:
    alpha:
      _type: categorical
      choices: [0.001, 0.01, 0.1, 1.0]

elasticnet:
  class_path: sklearn.linear_model.ElasticNet
  n_trials: 12              # 4 valores de α × 3 valores de l1_ratio → mais combinações possíveis
  params:
    alpha:
      _type: categorical
      choices: [0.001, 0.01, 0.1, 1.0]
    l1_ratio:
      _type: categorical
      choices: [0.2, 0.5, 0.8]

bayesian_ridge:
  class_path: sklearn.linear_model.BayesianRidge
  n_trials: 9               # 3 valores de alpha_1 × 3 de alpha_2, random search cobra um pouco mais
  params:
    alpha_1:
      _type: categorical
      choices: [0.000001, 0.0001, 0.01]
    alpha_2:
      _type: categorical
      choices: [0.000001, 0.0001, 0.01]

gradient_boosting:
  class_path: sklearn.ensemble.GradientBoostingRegressor
  n_trials: 50            # or even 30 if you’re really constrained
  params:
    n_estimators:
      _type: int
      low: 50
      high: 200

    learning_rate:
      _type: loguniform
      low: 0.05
      high: 0.2

    max_leaf_nodes:
      _type: categorical
      choices: [5, 10]

    max_features:
      _type: categorical
      choices: ["log2", "sqrt"]

# Boosters externos
xgboost:
  class_path: xgboost.XGBRegressor
  n_trials: 50              # número de rounds de tuning
  params:
    # 1. número de árvores — não ultrapassar 300
    n_estimators:
      _type: int
      low: 50
      high: 300

    # 2. shrinkage — focar em 0.05–0.2
    learning_rate:
      _type: loguniform
      low: 0.05
      high: 0.2

    # 3. profundidade das árvores — 2–3 opções
    max_depth:
      _type: categorical
      choices: [3, 6, 9]

    # 4. amostragem de instâncias
    subsample:
      _type: uniform
      low: 0.6
      high: 1.0

    # 5. amostragem de colunas por árvore
    colsample_bytree:
      _type: categorical
      choices: [0.6, 0.8, 1.0]

svr:
  class_path: sklearn.svm.SVR
  n_trials: 10
  params:
    C:
      _type: loguniform
      low: 0.1
      high: 10.0
    epsilon:
      _type: loguniform
      low: 0.001
      high: 0.5
    kernel:
      _type: categorical
      choices: ["rbf"]

gaussian_process:
  class_path: sklearn.gaussian_process.GaussianProcessRegressor
  n_trials: 6
  params:
    alpha:
      _type: loguniform
      low: 0.000001
      high: 0.1

mlp:
  class_path: sklearn.neural_network.MLPRegressor
  n_trials: 12
  params:
    hidden_layer_sizes:
      _type: categorical
      choices: [[32], [64], [32,16]]
    alpha:
      _type: loguniform
      low: 0.00001
      high: 0.01
    learning_rate_init:
      _type: loguniform
      low: 0.0001
      high: 0.01
    max_iter:
      _type: int
      low: 200
      high: 1000
